{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,GradientBoostingClassifier,GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you a fan of Google or Microsoft?</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Both are excellent technology they are helpfu...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Google provides online related services and p...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yeah, their services are good. I'm just not a...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id                                            message  \\\n",
       "0                1              Are you a fan of Google or Microsoft?   \n",
       "1                1   Both are excellent technology they are helpfu...   \n",
       "2                1   I'm not  a huge fan of Google, but I use it a...   \n",
       "3                1   Google provides online related services and p...   \n",
       "4                1   Yeah, their services are good. I'm just not a...   \n",
       "\n",
       "                 sentiment  \n",
       "0   Curious to dive deeper  \n",
       "1   Curious to dive deeper  \n",
       "2   Curious to dive deeper  \n",
       "3   Curious to dive deeper  \n",
       "4   Curious to dive deeper  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"D:/deva/topical_chat.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188373</th>\n",
       "      <td>8628</td>\n",
       "      <td>wow  it does not seem like that long  since i...</td>\n",
       "      <td>Surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188374</th>\n",
       "      <td>8628</td>\n",
       "      <td>i havent seen that episode  i might google it...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188375</th>\n",
       "      <td>8628</td>\n",
       "      <td>i don t think i have either  that s an insane...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188376</th>\n",
       "      <td>8628</td>\n",
       "      <td>i did  my little brother used to love thomas ...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188377</th>\n",
       "      <td>8628</td>\n",
       "      <td>it did  ringo starr  george carlin  and alec ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        conversation_id                                            message  \\\n",
       "188373             8628   wow  it does not seem like that long  since i...   \n",
       "188374             8628   i havent seen that episode  i might google it...   \n",
       "188375             8628   i don t think i have either  that s an insane...   \n",
       "188376             8628   i did  my little brother used to love thomas ...   \n",
       "188377             8628   it did  ringo starr  george carlin  and alec ...   \n",
       "\n",
       "                      sentiment  \n",
       "188373                Surprised  \n",
       "188374   Curious to dive deeper  \n",
       "188375   Curious to dive deeper  \n",
       "188376                    Happy  \n",
       "188377                  Neutral  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"message\"]=data[\"message\"].apply(lambda x:x.lower())\n",
    "data[\"message\"]=data[\"message\"].apply(lambda x:re.sub(\"[^a-zA-z0-9\\s]\",\" \",x))\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem  import WordNetLemmatizer\n",
    "from wordcloud import WordCloud,ImageColorGenerator,STOPWORDS\n",
    "from os import path, getcwd\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re,string,unicodedata\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_word = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=re.sub(r\"#\\w+\",\" \",text)\n",
    "    text=re.sub(r\"@\\w+\",\" \",text)\n",
    "    text=re.sub(r\"\\d+\",\" \",text)\n",
    "    text=re.sub(r\"http\\S+\",\" \",text)\n",
    "    text=re.sub(\"r<.*?>\",\" \",text)\n",
    "    text=text.split()\n",
    "    text=\" \".join([word for word in text if not word in stop_word])\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188373</th>\n",
       "      <td>8628</td>\n",
       "      <td>wow seem like long since mentioned simpsons ea...</td>\n",
       "      <td>Surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188374</th>\n",
       "      <td>8628</td>\n",
       "      <td>havent seen episode might google later yeah si...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188375</th>\n",
       "      <td>8628</td>\n",
       "      <td>think either insane amount episodes ever see t...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188376</th>\n",
       "      <td>8628</td>\n",
       "      <td>little brother used love thomas tank engine re...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188377</th>\n",
       "      <td>8628</td>\n",
       "      <td>ringo starr george carlin alec baldwin narrated</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        conversation_id                                            message  \\\n",
       "188373             8628  wow seem like long since mentioned simpsons ea...   \n",
       "188374             8628  havent seen episode might google later yeah si...   \n",
       "188375             8628  think either insane amount episodes ever see t...   \n",
       "188376             8628  little brother used love thomas tank engine re...   \n",
       "188377             8628    ringo starr george carlin alec baldwin narrated   \n",
       "\n",
       "                      sentiment  \n",
       "188373                Surprised  \n",
       "188374   Curious to dive deeper  \n",
       "188375   Curious to dive deeper  \n",
       "188376                    Happy  \n",
       "188377                  Neutral  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"message\"]=data[\"message\"].apply(lambda x:clean(x))\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y=np.array(data[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 5000\n",
    "tokenizer = Tokenizer(nb_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['message'].values)\n",
    "x = tokenizer.texts_to_sequences(data['message'].values)\n",
    "x = pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine=RandomForestClassifier(n_estimators=50)\n",
    "machine.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.0475427066855"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine.score(x,y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188368</th>\n",
       "      <td>8628</td>\n",
       "      <td>must definetely makes think twice well love tv...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188369</th>\n",
       "      <td>8628</td>\n",
       "      <td>like adult animated series like archer south p...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188370</th>\n",
       "      <td>8628</td>\n",
       "      <td>hilarious love shows also like pokemon mostly ...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188371</th>\n",
       "      <td>8628</td>\n",
       "      <td>believe many many years</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188372</th>\n",
       "      <td>8628</td>\n",
       "      <td>like pokemon episodes around years since first...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188373</th>\n",
       "      <td>8628</td>\n",
       "      <td>wow seem like long since mentioned simpsons ea...</td>\n",
       "      <td>Surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188374</th>\n",
       "      <td>8628</td>\n",
       "      <td>havent seen episode might google later yeah si...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188375</th>\n",
       "      <td>8628</td>\n",
       "      <td>think either insane amount episodes ever see t...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188376</th>\n",
       "      <td>8628</td>\n",
       "      <td>little brother used love thomas tank engine re...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188377</th>\n",
       "      <td>8628</td>\n",
       "      <td>ringo starr george carlin alec baldwin narrated</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        conversation_id                                            message  \\\n",
       "188368             8628  must definetely makes think twice well love tv...   \n",
       "188369             8628  like adult animated series like archer south p...   \n",
       "188370             8628  hilarious love shows also like pokemon mostly ...   \n",
       "188371             8628                            believe many many years   \n",
       "188372             8628  like pokemon episodes around years since first...   \n",
       "188373             8628  wow seem like long since mentioned simpsons ea...   \n",
       "188374             8628  havent seen episode might google later yeah si...   \n",
       "188375             8628  think either insane amount episodes ever see t...   \n",
       "188376             8628  little brother used love thomas tank engine re...   \n",
       "188377             8628    ringo starr george carlin alec baldwin narrated   \n",
       "\n",
       "                      sentiment  \n",
       "188368   Curious to dive deeper  \n",
       "188369                    Happy  \n",
       "188370   Curious to dive deeper  \n",
       "188371   Curious to dive deeper  \n",
       "188372                    Happy  \n",
       "188373                Surprised  \n",
       "188374   Curious to dive deeper  \n",
       "188375   Curious to dive deeper  \n",
       "188376                    Happy  \n",
       "188377                  Neutral  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Curious to dive deeper', ' Happy', ' Curious to dive deeper',\n",
       "       ' Curious to dive deeper', ' Happy', ' Surprised',\n",
       "       ' Curious to dive deeper', ' Curious to dive deeper', ' Happy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine.predict(x[188368:188377])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>message</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fan google microsoft</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>excellent technology helpful many ways securit...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>huge fan google use lot think monopoly sense</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>google provides online related services produc...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah services good fan intrusive personal lives</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>google leading alphabet subsidiary continue um...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>know google hundreds live goats cut grass past</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>interesting google provide chrome os light wei...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>like google chrome use well browser</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>yes google biggest search engine google servic...</td>\n",
       "      <td>Curious to dive deeper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id                                            message  \\\n",
       "0                1                               fan google microsoft   \n",
       "1                1  excellent technology helpful many ways securit...   \n",
       "2                1       huge fan google use lot think monopoly sense   \n",
       "3                1  google provides online related services produc...   \n",
       "4                1    yeah services good fan intrusive personal lives   \n",
       "5                1  google leading alphabet subsidiary continue um...   \n",
       "6                1     know google hundreds live goats cut grass past   \n",
       "7                1  interesting google provide chrome os light wei...   \n",
       "8                1                like google chrome use well browser   \n",
       "9                1  yes google biggest search engine google servic...   \n",
       "\n",
       "                 sentiment  \n",
       "0   Curious to dive deeper  \n",
       "1   Curious to dive deeper  \n",
       "2   Curious to dive deeper  \n",
       "3   Curious to dive deeper  \n",
       "4   Curious to dive deeper  \n",
       "5   Curious to dive deeper  \n",
       "6   Curious to dive deeper  \n",
       "7   Curious to dive deeper  \n",
       "8   Curious to dive deeper  \n",
       "9   Curious to dive deeper  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Curious to dive deeper', ' Curious to dive deeper',\n",
       "       ' Curious to dive deeper', ' Curious to dive deeper',\n",
       "       ' Curious to dive deeper', ' Curious to dive deeper',\n",
       "       ' Curious to dive deeper', ' Curious to dive deeper',\n",
       "       ' Curious to dive deeper'], dtype=object)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine.predict(x[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Happy'], dtype=object)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine.predict(x[188376:188377])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
